<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="utf-8">
    <title>Li Yuhao&#39;s Home</title>
    <link rel="stylesheet" href="/style/style.css"/>
    <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css">
</head>

<body>

<div class="wrap">
        
    <nav class="NavigationBar">
        <table>
            <tr>
            <td class="TitleTar">
                <div class="SiteTitle">
                    <a href="/">Li Yuhao&#39;s Home</a>
                </div>
            </td>
            <td>
                <div class="SiteNavigation">

                        <a href="/research/"><i class="fa fa-graduation-cap" aria-hidden="true"></i> Research </a> &emsp;

                        <a href="/note/"><i class="fa fa-pencil-square-o" aria-hidden="true"></i> Note </a> &emsp;

                        <a href="/share/"><i class="fa fa-files-o" aria-hidden="true"></i> Share </a> &emsp;

                        <a href="/file/"><i class="fa fa-folder-open-o" aria-hidden="true"></i> File </a> &emsp;
                    
                </div>
            </td>
            </tr>
        </table>
    </nav>

  
    <div class="MainPage">

        <h1>Research</h1>

        <hr>

        </br>

        <blockquote>

        <h2>Spin Glass Model of In-Context Learning</h2>

        <p><b>Abstract:</b>&ensp;Large language models show a surprising in-context learning ability - being able to use a prompt to form a prediction for a query, yet without additional training, in stark contrast to old-fashioned supervised learning. Providing a mechanistic interpretation and linking the empirical phenomenon to physics are thus challenging and remain unsolved. We study a simple yet expressive transformer with linear attention, and map this structure to a spin glass model with real-valued spins, where the couplings and fields explain the intrinsic disorder in data. The spin glass model explains how the weight parameters interact with each other during pre-training, and most importantly why an unseen function can be predicted by providing only a prompt yet without training. Our theory reveals that for single instance learning, increasing the task diversity leads to the emergence of the in-context learning, by allowing the Boltzmann distribution to converge to a unique correct solution of weight parameters. Therefore the pre-trained transformer displays a prediction power in a novel prompt setting. The proposed spin glass model thus establishes a foundation to understand the empirical success of large language models.</p>

        <p>The preprint can be found on <a href="https://arxiv.org/abs/2408.02288" target="_blank">arXiv</a>.

        </blockquote>	

        </br>    

        <blockquote>

        <h2>Generalization Error of Perceptron With Continuous Weight in Binary Classification Task</h2>

        <p><b>Abstract:</b>&ensp;In the teacher-student model and Bayesian optimal framework, we analyze the generalization error of perceptrons with continuous weights in binary classification tasks. Firstly, we derive the generalized approximate message passing (GAMP) equation based on the belief propagation (BP) equation, and get the generalization errors of different data densities through iteration. Then, the state evolution (SE) equation is derived based on the messaging equation, and the theoretical curve of generalization error varying with data density is obtained through iteration, which is consistent with the results obtained by using the generalized messaging equation. Finally, we use the replica method to calculate and analyze, verify the results of the state evolution equation, and analyze the asymptotic behavior of generalization error when the data density tends to infinity.</p>

        <p>This project was awarded excellent in the final defense of College Students' Innovative Entrepreneurial Training Plan Program in 2023 in the School of Physics of Sun Yat-sen University.</p>

        <p>A pdf version of the report can be downloaded <a href="/research/file/Perceptron_v2_20231106.pdf" target="_blank">here</a>, the version of the ppt presented at the group meeting can be downloaded <a href="/research/file/PPT_Perceptron_20230617.pdf" target="_blank">here</a>, and the ppt version of the CSIETPP conclusion defense report can be downloaded <a href="/research/file/PPT_Perceptron_20231213.pdf" target="_blank">here</a>.</p> 

        </blockquote>
    
    </div>


</div>


<script language=javascript>
    function siteTime(){
        window.setTimeout("siteTime()", 1000);
        var seconds = 1000;
        var minutes = seconds * 60;
        var hours = minutes * 60;
        var days = hours * 24;
        var years = days * 365;
        var today = new Date();
        var todayYear = today.getFullYear();
        var todayMonth = today.getMonth()+1;
        var todayDate = today.getDate();
        var todayHour = today.getHours();
        var todayMinute = today.getMinutes();
        var todaySecond = today.getSeconds();
        var t1 = Date.UTC(2024,04,12,14,00,00);
        var t2 = Date.UTC(todayYear,todayMonth,todayDate,todayHour,todayMinute,todaySecond);
        var diff = t2-t1;
        var diffDays = Math.floor((diff/days));
        document.getElementById("sitetime").innerHTML=diffDays;
    }
    siteTime();
</script>

<div class="SiteFooter">
    This site has been running for <span id="sitetime"></span> days.
    <br>
    Copyright &copy; 2024 Li Yuhao. All Rights Reserved.
</div>

</body>

</html>
